# C10K和C10M
#### 并发请求问题
* 怎样在一个线程内处理多个请求，也就是要在一个线程内响应多个网络 I/O
* 怎么更节省资源地处理客户请求，也就是要用更少的线程来服务这些请求

#### 惊群问题
* accept() 和 epoll_wait() 调用，还存在一个惊群的问题。换句话说，当网络 I/O 事件发生时，多个进程被同时唤醒，但实际上只有一个进程来响应这个事件，其他被唤醒的进程都会重新休眠。
* EPOLLEXCLUSIVE

#### nginx解决惊群问题
*  Nginx 在每个 worker 进程中，都增加一个了全局锁（accept_mutex）。这些 worker 进程需要首先竞争到锁，只有竞争到锁的进程，才会加入到 epoll 中，这样就确保只有一个 worker 子进程被唤醒
* 所有的进程都监听相同的接口，并且开启 SO_REUSEPORT 选项，由内核负责将请求负载均衡到这些监听进程中去。这一过程如下图所示。
* ![90df0945f6ce5c910ae361bf2b135bbd](media/15498656013403/90df0945f6ce5c910ae361bf2b135bbd.png)
* 由内核确保只有一个进程被唤醒

#### C1000K
* 文件描述符的数量
* 连接状态的跟踪（CONNTRACK）
* 网络协议栈的缓存大小
* 多队列网卡
* 中断负载均衡
* CPU 绑定
* RPS/RFS（软中断负载均衡到多个 CPU 核上
* 网络包的处理卸载（Offload）

#### C10M
* DPDK
* 是用户态网络的标准。它跳过内核协议栈，直接由用户态进程通过轮询的方式，来处理网络接收。
![998fd2f52f0a48a910517ada9f2bb23a](media/15498656013403/998fd2f52f0a48a910517ada9f2bb23a.png)
* 实际工作时间远远高于查询时间
* 在 PPS 非常高的场景中，查询时间比实际工作时间少了很多，绝大部分时间都在处理网络包，而跳过内核协议栈后，就省去了繁杂的硬中断、软中断再到 Linux 网络协议栈逐层处理的过程，应用程序可以针对应用的实际场景，有针对性地优化网络包的处理逻辑，而不需要关注所有的细节。
* 大页、CPU 绑定、内存对齐、流水线并发等多种机制

#### XDP
* Linux 内核提供的一种高性能网络数据路径。它允许网络包，在进入内核协议栈之前，就进行处理，也可以带来更高的性能。XDP 底层跟我们之前用到的 bcc-tools 一样，都是基于 Linux 内核的 eBPF 机制实现的。
* 